{"version":"1","records":[{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nSee the \n\nCookbook Contributor’s Guide for step-by-step instructions on how to create your new Cookbook and get it hosted on the \n\nPythia Cookbook Gallery!\n\nThis Project Pythia Cookbook covers how to employ a simple AI model to predict hurricane intensity using IBTRACKS and ERA5 environmental variables.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"content":"Here, you will learn how to run a simple AI model to predict hurricane intensity. You will first preprocess ERA5 environmental variables associated with each Tropical Storm in the IBTRACKS dataset. Then, you will use this preprocessed data to create and train an AI model to predict future cyclone intensity. You will gain experience working primarily with xarray and ...","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"content":"Nirmal Alex, \n\nMatthew Lynne, etc. Acknowledge primary content authors here","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"type":"lvl3","url":"/#introduction","position":10},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"content":"Read in the data that will be used, including ERA5 and IBTRACKS.","type":"content","url":"/#introduction","position":11},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"type":"lvl3","url":"/#data-preprocessing","position":12},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"content":"Process the data into a format that is appropriate for the AI model.","type":"content","url":"/#data-preprocessing","position":13},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"type":"lvl3","url":"/#training-the-model","position":14},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"content":"","type":"content","url":"/#training-the-model","position":15},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"Model Setup"},"type":"lvl1","url":"/notebooks/model","position":0},{"hierarchy":{"lvl1":"Model Setup"},"content":"\n\n","type":"content","url":"/notebooks/model","position":1},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/model#overview","position":2},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Overview"},"content":"This notebook is for setting up a simple CNN-LSTM model to predict event-wise hurricane intensity using the prreproceesed data from the era5_preprocessing.ipynb notebook.\n\nData Preparation: Load and preprocess the data.\n\nModel Definition: Define a CNN-LSTM model.\n\nModel Training: Train the model on the prepared data.\n\n","type":"content","url":"/notebooks/model#overview","position":3},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/model#prerequisites","position":4},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nIntro to TensorFlow\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/model#prerequisites","position":5},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/model#imports","position":6},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Imports"},"content":"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport xarray as xr\n\n","type":"content","url":"/notebooks/model#imports","position":7},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Load the hurricane wise variable dataset"},"type":"lvl2","url":"/notebooks/model#load-the-hurricane-wise-variable-dataset","position":8},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Load the hurricane wise variable dataset"},"content":"\n\nIn this section,we will load the preprocessed dataset containing hurricane-wise environmental variables. This dataset is essential for training our CNN-LSTM model to predict hurricane intensity.\n\n# load the preprocessed dataset\nmodel_input = xr.open_dataset('../test_folder/input_predictands.nc')\nmodel_input\n\n\n","type":"content","url":"/notebooks/model#load-the-hurricane-wise-variable-dataset","position":9},{"hierarchy":{"lvl1":"Model Setup","lvl3":"Input Data prerpocessing steps","lvl2":"Load the hurricane wise variable dataset"},"type":"lvl3","url":"/notebooks/model#input-data-prerpocessing-steps","position":10},{"hierarchy":{"lvl1":"Model Setup","lvl3":"Input Data prerpocessing steps","lvl2":"Load the hurricane wise variable dataset"},"content":"Nan and padded values will be set to zero\n\nSet the train and test split\n\nnormalize using the MinMaxScaler\n\nrandom shuffle for generalization\n\n# remove the nan values and set it \nmodel_input = model_input.fillna(0)\n\n# selecting the predictors (X) and expanding the dimensions\n\nX_data = model_input[['u','v','vo','speed_shear','sp','r','cor_params']].to_array(dim='variable')\n\nprint(f'Dimensions are , features: {X_data.shape[0]}, Event: {X_data.shape[1]}, time(lead): {X_data.shape[2]}, lat: {X_data.shape[3]}, lon: {X_data.shape[4]}')\n\nX_data = X_data.transpose('id', 'lead', 'y','x','variable')\n\nprint(f'X_data dimensions are: Event: {X_data.shape[0]}, time(lead): {X_data.shape[1]}, lat: {X_data.shape[2]}, lon: {X_data.shape[3]}, features: {X_data.shape[4]}')\n\n# selecting the target variable (y)\nY_data = model_input['target']\n\n# expanded the dimensions of Y_data to match the expected input shape for the model\nY_data = np.expand_dims(Y_data, axis=-1)\n\nprint(f'Target dimensions are: Event: {Y_data.shape[0]}, time(lead): {Y_data.shape[1]}, windspeed: {Y_data.shape[2]}')\n\n## 80% train and 20% test split\n\n## random shuffled the events and split the data into training and testing sets\nX_train , X_test , Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=1)\n\n\nX_train_data = X_train.values\nX_test_data = X_test.values\nY_train_data = Y_train.values\nY_test_data = Y_test.values\n\n\n\n\nx_train_scaler = MinMaxScaler()\nx_test_scaler = MinMaxScaler()\n\ny_train_scaler = MinMaxScaler()\ny_test_scaler = MinMaxScaler()\n\nx_train_scaled = x_train_scaler.fit_transform(X_train_data.reshape(-1, X_train_data.shape[-1])).reshape(X_train_data.shape)\ny_train_scaled = y_train_scaler.fit_transform(Y_train_data.reshape(-1,1)).reshape(Y_train_data.shape)\n\nx_test_scaled = x_test_scaler.fit_transform(X_test_data.reshape(-1, X_test_data.shape[-1])).reshape(X_test_data.shape)\ny_test_scaled = y_test_scaler.fit_transform(Y_test_data.reshape(-1,1)).reshape(Y_test_data.shape)\n\n\ndef masked_mse(y_true, y_pred):\n    mask = tf.cast(tf.not_equal(y_true, 0.0), tf.float32)\n    squared_error = tf.square(y_true - y_pred)\n    masked_loss = tf.reduce_sum(squared_error * mask) / (tf.reduce_sum(mask) + 1e-6)\n    return masked_loss\n\nmodel = Sequential()\nmodel.add(TimeDistributed(\n    Conv2D(16, (3, 3), activation='relu', padding='same'),\n    input_shape=(140, 5, 5, 7)\n))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(64, return_sequences=True))  # <--- important!\nmodel.add(TimeDistributed(Dense(1)))\n\nmodel.summary()\n\n\nvisualkeras.layered_view(model)\n\nmodel.compile(optimizer='adam', metrics=['mae'] , loss=masked_mse)\n\nmodel.fit(x_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2)\n\npredict_x = model.predict(x_test_scaled)\n\ndef invert_add_meta(data,scalar,xr_data):\n    data = scalar.inverse_transform(data.reshape(-1 ,1)).reshape(xr_data.shape)\n    added_meta = xr.DataArray(data, coords=xr_data.coords, dims=xr_data.dims)\n    return added_meta\n\npredicted_wind_speed = invert_add_meta(predict_x, y_test_scaler, Y_test)\n\n\n\n\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nMyST Syntax Overview for MyST-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/model#input-data-prerpocessing-steps","position":11},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/model#last-section","position":12},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Last Section"},"content":"You can add \n\nadmonitions using MyST syntax:\n\nNote\n\nYour relevant information here!\n\nSome other admonitions you can put in (\n\nthere are 10 total):\n\nHint\n\nA helpful hint.\n\nWarning\n\nBe careful!\n\nDanger\n\nScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/model#last-section","position":13},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/model#summary","position":14},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/model#summary","position":15},{"hierarchy":{"lvl1":"Model Setup","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/model#whats-next","position":16},{"hierarchy":{"lvl1":"Model Setup","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/model#whats-next","position":17},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/model#resources-and-references","position":18},{"hierarchy":{"lvl1":"Model Setup","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/model#resources-and-references","position":19},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"type":"lvl1","url":"/notebooks/era5-preprocessing","position":0},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"content":"\n\n","type":"content","url":"/notebooks/era5-preprocessing","position":1},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/era5-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"content":"Here, we will use the processed IBTRACKS data to select ERA5 environmental variables associated with each cyclone.\n\n","type":"content","url":"/notebooks/era5-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/era5-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: ~15 minntes\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/era5-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\nimport numpy as np\nimport dask\n\n","type":"content","url":"/notebooks/era5-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":8},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"content":"\n\nIn this section, we will select ERA5 data within a 5x5 latitude/longitude grid centered at each cyclone center at each time step in our dataset. We will then have to pad the data to account for instances in which grid cells occur over land.\n\ninput_dsets = xr.open_dataset('~/Data/final_proc_5yr_6h.nc')\n\nHint\n\nThe coriolis parameter is a function of latitude only. However, cyclones tend to move in preferred directions based on latitude and in turn the magnitude of this parameter. This is why the coriolis parameter is chosen to be one of the predictor variables of our AI model. This variable is calculated below.\n\n# calculating coriolis parameter \ncor_parms =  2 * 7.29 * 1e-5 * np.sin(np.radians(input_dsets['latitude']))\n\ninput_dsets['cor_params'] = xr.DataArray(cor_parms,\n                                            name='cor_params'\n                                            ).broadcast_like(input_dsets['r'])\n\nib_data_processed_6h = pd.read_csv('../test_folder/ib_data_processed_6h.csv')\n\nHint\n\nWhen training our AI model, we want all cyclones to have the same number of time steps. Realistically this does not happen in the real world. Therefore, we must pad each cyclone track with “dummy” values until the lifespan of the cyclone is the same as that of the longest lasting cyclone in our dataset.\n\nfinal_data = []\nmax_len = ib_data_processed_6h.groupby('id').size().max()  # assuming max length is 3 hours per storm\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":9},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictors for each cyclone"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-predictors-for-each-cyclone","position":10},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictors for each cyclone"},"content":"Here we can move on to our second objective, to explicitly edit the predictors that will be used by the machine learning model. We wish to center each cyclone within a 5x5 grid at each time step. We will then select the data at each grid cell for each variable of interest including sea surface temperatures, 500 hPa relatice humidity, pressure, vertical wind shear, 850 hPa relative vorticity, and the coriolis parameter.\n\nfor id_number,group in ib_data_processed_6h.groupby('id'):\n    events_data = []\n    for index,row in group.iterrows():\n        lat = int(row['LAT'])\n        lon = int(row['LON'])\n        time = row['datetime']\n        \n        #We want data in a 5x5 latitude/longitude grid centered on the cyclone latitude/longitude\n        latmin = lat - 2\n        latmax = lat + 2\n        lonmin = lon - 2\n        lonmax = lon + 2\n        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n        \n            \n        final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n        final_xr['x'] = np.arange(0,final_xr.sizes['x'])\n        final_xr['y'] = np.arange(0,final_xr.sizes['y'])\n        \n        # fill NaN values with zeros along the x and y dimensions\n        for jj in final_xr.data_vars:\n            final_xr[jj].fillna(0)  # Fill NaN values\n        \n        #Recall that we are trying to predict the wind speed.\n        #Hence, our target is USA_WIND\n        final_xr['target'] = row['USA_WIND']    \n        events_data.append(final_xr)\n    \n    final_event = xr.concat(events_data,dim='time')\n    \n    #Pad data with zeros up to the maximum time\n    if len(final_event.time) <= max_len:\n        new_time = pd.date_range(start=final_event['time'].min().values, periods=max_len ,freq='6h')\n        padded_data = final_event.reindex(time=new_time, fill_value=0.0)\n    else:\n        padded_data = final_event\n    \n    lead_time = np.arange(0,max_len*6 ,6)\n    padded_data['lead'] = ('time', lead_time)\n    padded_data = padded_data.assign_coords({'lead': padded_data['lead'].astype(int)})\n    \n    # swap time and lead dimensions\n    padded_data = padded_data.swap_dims({'time': 'lead'})\n    padded_data['id'] = id_number \n    padded_data = padded_data.set_coords('id')\n    \n    # convert the time dimension to a variable\n    final_data.append(padded_data)\n\nfinal_input_padded = xr.concat(final_data, dim='id')\nfinal_input_padded\n\nNote\n\nRecall that our “target” variable, or the variable we want to predict is the wind speed. We take the wind speed from ERA5 to initially train our model.\n\nfinal_input_padded.to_netcdf('~/ml-hurricane-intensity/test_folder/input_predictands.nc')\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-predictors-for-each-cyclone","position":11},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/era5-preprocessing#summary","position":12},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"content":"Here, we selected and edited ERA5 data associated with the cyclones at each time step in our dataset. This involved gathering data for each variable of interest within a 5x5 grid. We also needed to be sure to mask out all grid cells corresponding to land as our AI model will only take into account grid cells over water.","type":"content","url":"/notebooks/era5-preprocessing#summary","position":13},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/era5-preprocessing#whats-next","position":14},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"content":"We have now officially preprocessed all of our data! Next, we will test each variable of interest to get a sense of how well it can act as a predictor for cyclone intensity. After this, we will begin setting up our AI model!","type":"content","url":"/notebooks/era5-preprocessing#whats-next","position":15},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"type":"lvl1","url":"/notebooks/ibtrack-preprocessing","position":0},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"content":"The first step of any machine learning algorithm is to first load in, filter, and then process the necessary data. Here, we will load in the hurricane track data from IBTRACKS and then filter the tracks by requirements that we will set.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":8},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"content":"\n\nFirst, read in the data as a csv file\n\nib_data = '~/ml-hurricane-intensity/test_folder/ibtracs.NA.list.v04r00.csv'\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":9},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":10},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"content":"The following method will set custom ranges for the time period and spatial domain of analysis.\n\ndef process_ibrack(ib_loc, periods=[2000,2005]):\n\n    #Read in the IBTRACKS data\n    read_ib_data = pd.read_csv(ib_loc,keep_default_na=False)\n    \n    #Get the units for each column in read_ib_data\n    units = read_ib_data.iloc[0,:]\n\n    #Get data or the remainder of read_ib_data\n    ib_original_dft = read_ib_data.iloc[1:,:]\n\n    #Set a custom date and time range based on user choosing\n    ib_original_dft['datetime'] = pd.to_datetime(ib_original_dft['ISO_TIME'],format='%Y-%m-%d %H:%M:%S')\n    year_mask = (ib_original_dft['datetime'] > f'{periods[0]}-1-1') & (ib_original_dft['datetime'] <= f'{periods[1]}-11-30')\n    ib_new_period = ib_original_dft[year_mask][ib_original_dft['BASIN'] == 'NA']\n\n    #Only use cyclones over the North Atlantic basin\n    #This can be changed to include more cyclones outside of the Northeast Atlantic\n    def only_na_basin(df):\n        lon_wise = df.sort_values(by='datetime')\n        if lon_wise['LON'].iloc[0] > -55:\n            return df\n        else:\n            return None\n\n    #Get the number of time steps in each cyclone, or event\n    only_neatlantic = ib_new_period.groupby('SID').apply(only_na_basin).reset_index(drop=True)\n    counts = only_neatlantic.groupby('SID').count().iloc[:,0]\n    \n    #Get cyclones that last at least 12 time steps\n    counts_12 = counts[counts > 12].index\n    persist_storms = ib_new_period[ib_new_period['SID'].isin(counts_12)]\n    persist_storms['month']= persist_storms['datetime'].dt.month\n    \n    # mask out land points\n    def mask_lands(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat = ordered_df['LAT']\n        lon = ordered_df['LON']\n        ocean_mask = pd.Series(globe.is_ocean(lat=lat,lon=lon))\n        idx_false = ocean_mask.idxmin()\n        if idx_false == 0:\n            return df\n        else:\n            land_mask = ocean_mask.iloc[:idx_false]\n            final_masked = ordered_df.iloc[:idx_false,:]\n            return final_masked\n   \n    # filter extratropical parts of the storm tracks\n    def filter_ET(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat_filter = ordered_df['LAT'] <= 35\n        filter_df = ordered_df[lat_filter]\n        return filter_df\n    \n    exclude_et = persist_storms.groupby('SID').apply(filter_ET).reset_index(drop=True)\n\n    final_dft = exclude_et.groupby('SID').apply(mask_lands).reset_index(drop=True)\n\n    return final_dft\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":11},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":12},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"content":"\n\nib_data_processed = process_ibrack(ib_data,periods=[2000,2005])\nib_data_processed['LAT'] = ib_data_processed['LAT'].astype(float)\nib_data_processed['LON'] = ib_data_processed['LON'].astype(float)\nib_data_processed['USA_WIND'] = ib_data_processed['USA_WIND'].astype(float)\nib_data_processed['datetime'] = pd.to_datetime(ib_data_processed['datetime'],format='%Y-%m-%d %H:%M:%S')\nib_data_processed['SID'] = ib_data_processed['SID'].astype(str)\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":13},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":14},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"content":"\n\nHint\n\nUp until now, we have primarily been grouping the IBTRACKS data using SID. Starting in this block, we will begin grouping the IBTRACKS data by id. This is done since SID may erroneously connect some cyclone tracks at adjacent time steps, while id appears to keep them separate.\n\nib_data_processed['id'] = ib_data_processed['SID'].astype('category')\nib_data_processed['id'] = ib_data_processed['id'].cat.codes\nreq_cols = ['datetime','LAT','LON','USA_WIND','id']\n\n# groupby datetime 6h \nib_data_processed['datetime'] = ib_data_processed['datetime'].dt.floor('6h')\n\nib_data_processed_6h = ib_data_processed[req_cols].groupby('datetime').mean().reset_index()\n\n# copy the SID based on the id\n\nib_data_processed_6h['SID'] = ib_data_processed.groupby('datetime')['SID'].first().values\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":15},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":16},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"content":"\n\nib_data_processed_6h\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":17},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":18},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"content":"\n\nNote\n\nYou may notice some instances in which the cyclone track hits the top of the boundary, and then appears to erroneously move horizontally over a long distance. Not to worry! This occurs since we discount all cyclones that move north of our boundary. Thus, the plot below will connect the two cyclone positions before and after it leaves the domain. The data is however correct.\n\ndef plot_tracks(filtered_ib):\n    filtered_ib['datetime'] = pd.to_datetime(filtered_ib['datetime'])\n\n    events = filtered_ib.groupby('id')\n    \n    fig,ax = plt.subplots(figsize=(20,10),subplot_kw={\"projection\": ccrs.PlateCarree()})\n    \n    ax.add_feature(cfeature.COASTLINE)\n\n    ax.add_feature(cfeature.BORDERS)\n\n\n    ax.coastlines()\n    \n    ax.add_feature(cfeature.LAND)\n    ax.add_feature(cfeature.OCEAN)\n    \n    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n  \n    for event_num , event in events:\n        lon = event['LON'].values\n        lat = event['LAT'].values\n\n        vertices = [(lo, la) for lo, la in zip(lon, lat)]\n        codes = [Path.MOVETO]\n        [codes.append(Path.LINETO) for index in range(0, len(event) - 1)]\n\n        path = Path(vertices, codes)\n        \n        patch = patches.PathPatch(path, lw = 1, fc = 'none', path_effects = [patheffects.withStroke(linewidth=2.5, foreground=\"black\")], zorder = 5)\n\n        ax.add_patch(patch)\n        \n        ax.set_xlim(-100, 0)\n        ax.set_ylim(5, 35)\n    return fig,ax\n\nplot_tracks(ib_data_processed_6h)\n\nib_data_processed_6h.to_csv('../test_folder/ib_data_processed_6h.csv')\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":19},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#summary","position":20},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"content":"Here, we appropriately edited the cyclone track data over our space and time domain of interest. We have also created an output file that will be useful for eventual creation and training of our machine learning model.","type":"content","url":"/notebooks/ibtrack-preprocessing#summary","position":21},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/ibtrack-preprocessing#whats-next","position":22},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Next we will edit the ERA5 data. In particular, we will take our variables of interest that will be used to train the AI model and organize them for ingestion into the machine learning model.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#whats-next","position":23},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":24},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"content":"Image link: \n\nhttps://​smartcorp​.com​/blog​/what​-silicon​-valley​-bank​-can​-learn​-from​-supply​-chain​-planning​/attachment​/2​-scenarios​-used​-by​-the​-national​-weather​-service​-to​-predict​-hurricane​-tracks/","type":"content","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":25}]}