{"version":"1","records":[{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nSee the \n\nCookbook Contributor’s Guide for step-by-step instructions on how to create your new Cookbook and get it hosted on the \n\nPythia Cookbook Gallery!\n\nThis Project Pythia Cookbook covers how to employ a simple AI model to predict hurricane intensity using IBTRACKS and ERA5 environmental variables.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"content":"Here, you will learn how to run a simple AI model to predict hurricane intensity. You will first preprocess ERA5 environmental variables associated with each Tropical Storm in the IBTRACKS dataset. Then, you will use this preprocessed data to create and train an AI model to predict future cyclone intensity. You will gain experience working primarily with xarray and ...","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"content":"Nirmal Alex, \n\nMatthew Lynne, etc. Acknowledge primary content authors here","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"type":"lvl3","url":"/#introduction","position":10},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"content":"Read in the data that will be used, including ERA5 and IBTRACKS.","type":"content","url":"/#introduction","position":11},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"type":"lvl3","url":"/#data-preprocessing","position":12},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"content":"Process the data into a format that is appropriate for the AI model.","type":"content","url":"/#data-preprocessing","position":13},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"type":"lvl3","url":"/#training-the-model","position":14},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"content":"","type":"content","url":"/#training-the-model","position":15},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"type":"lvl1","url":"/notebooks/era5-preprocessing","position":0},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"content":"\n\n","type":"content","url":"/notebooks/era5-preprocessing","position":1},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/era5-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"content":"Here, we will use the processed IBTRACKS data to select ERA5 environmental variables associated with each cyclone.\n\n","type":"content","url":"/notebooks/era5-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/era5-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: ~15 minntes\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/era5-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\nimport numpy as np\nimport dask\n\n","type":"content","url":"/notebooks/era5-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":8},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"content":"\n\nIn this section, we will select ERA5 data within a 5x5 latitude/longitude grid centered at each cyclone center at each time step in our dataset. We will then have to pad the data to account for instances in which grid cells occur over land.\n\ninput_dsets = xr.open_dataset('~/Data/final_proc_5yr_6h.nc')\n\nHint\n\nThe coriolis parameter is a function of latitude only. However, cyclones tend to move in preferred directions based on latitude and in turn the magnitude of this parameter. This is why the coriolis parameter is chosen to be one of the predictor variables of our AI model. This variable is calculated below.\n\n# calculating coriolis parameter \ncor_parms =  2 * 7.29 * 1e-5 * np.sin(np.radians(input_dsets['latitude']))\n\ninput_dsets['cor_params'] = xr.DataArray(cor_parms,\n                                            name='cor_params'\n                                            ).broadcast_like(input_dsets['r'])\n\nib_data_processed_6h = pd.read_csv('../test_folder/ib_data_processed_6h.csv')\n\nHint\n\nWhen training our AI model, we want all cyclones to have the same number of time steps. Realistically this does not happen in the real world. Therefore, we must pad each cyclone track with “dummy” values until the lifespan of the cyclone is the same as that of the longest lasting cyclone in our dataset.\n\nfinal_data = []\nmax_len = ib_data_processed_6h.groupby('id').size().max()  # assuming max length is 3 hours per storm\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":9},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictors for each cyclone"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-predictors-for-each-cyclone","position":10},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictors for each cyclone"},"content":"Here we can move on to our second objective, to explicitly edit the predictors that will be used by the machine learning model. We wish to center each cyclone within a 5x5 grid at each time step. We will then select the data at each grid cell for each variable of interest including sea surface temperatures, 500 hPa relatice humidity, pressure, vertical wind shear, 850 hPa relative vorticity, and the coriolis parameter.\n\nfor id_number,group in ib_data_processed_6h.groupby('id'):\n    events_data = []\n    for index,row in group.iterrows():\n        lat = int(row['LAT'])\n        lon = int(row['LON'])\n        time = row['datetime']\n        \n        #We want data in a 5x5 latitude/longitude grid centered on the cyclone latitude/longitude\n        latmin = lat - 2\n        latmax = lat + 2\n        lonmin = lon - 2\n        lonmax = lon + 2\n        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n        \n            \n        final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n        final_xr['x'] = np.arange(0,final_xr.sizes['x'])\n        final_xr['y'] = np.arange(0,final_xr.sizes['y'])\n        \n        # fill NaN values with zeros along the x and y dimensions\n        for jj in final_xr.data_vars:\n            final_xr[jj].fillna(0)  # Fill NaN values\n        \n        #Recall that we are trying to predict the wind speed.\n        #Hence, our target is USA_WIND\n        final_xr['target'] = row['USA_WIND']    \n        events_data.append(final_xr)\n    \n    final_event = xr.concat(events_data,dim='time')\n    \n    #Pad data with zeros up to the maximum time\n    if len(final_event.time) <= max_len:\n        new_time = pd.date_range(start=final_event['time'].min().values, periods=max_len ,freq='6h')\n        padded_data = final_event.reindex(time=new_time, fill_value=0.0)\n    else:\n        padded_data = final_event\n    \n    lead_time = np.arange(0,max_len*6 ,6)\n    padded_data['lead'] = ('time', lead_time)\n    padded_data = padded_data.assign_coords({'lead': padded_data['lead'].astype(int)})\n    \n    # swap time and lead dimensions\n    padded_data = padded_data.swap_dims({'time': 'lead'})\n    padded_data['id'] = id_number \n    padded_data = padded_data.set_coords('id')\n    \n    # convert the time dimension to a variable\n    final_data.append(padded_data)\n\nfinal_input_padded = xr.concat(final_data, dim='id')\nfinal_input_padded\n\nNote\n\nRecall that our “target” variable, or the variable we want to predict is the wind speed. We take the wind speed from ERA5 to initially train our model.\n\nfinal_input_padded.to_netcdf('~/ml-hurricane-intensity/test_folder/input_predictands.nc')\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-predictors-for-each-cyclone","position":11},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/era5-preprocessing#summary","position":12},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"content":"Here, we selected and edited ERA5 data associated with the cyclones at each time step in our dataset. This involved gathering data for each variable of interest within a 5x5 grid. We also needed to be sure to mask out all grid cells corresponding to land as our AI model will only take into account grid cells over water.","type":"content","url":"/notebooks/era5-preprocessing#summary","position":13},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/era5-preprocessing#whats-next","position":14},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"content":"We have now officially preprocessed all of our data! Next, we will test each variable of interest to get a sense of how well it can act as a predictor for cyclone intensity. After this, we will begin setting up our AI model!","type":"content","url":"/notebooks/era5-preprocessing#whats-next","position":15},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"type":"lvl1","url":"/notebooks/ibtrack-preprocessing","position":0},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"content":"The first step of any machine learning algorithm is to first load in, filter, and then process the necessary data. Here, we will load in the hurricane track data from IBTRACKS and then filter the tracks by requirements that we will set.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":8},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"content":"\n\nFirst, read in the data as a csv file\n\nib_data = '~/ml-hurricane-intensity/test_folder/ibtracs.NA.list.v04r00.csv'\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":9},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":10},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"content":"The following method will set custom ranges for the time period and spatial domain of analysis.\n\ndef process_ibrack(ib_loc, periods=[2000,2005]):\n\n    #Read in the IBTRACKS data\n    read_ib_data = pd.read_csv(ib_loc,keep_default_na=False)\n    \n    #Get the units for each column in read_ib_data\n    units = read_ib_data.iloc[0,:]\n\n    #Get data or the remainder of read_ib_data\n    ib_original_dft = read_ib_data.iloc[1:,:]\n\n    #Set a custom date and time range based on user choosing\n    ib_original_dft['datetime'] = pd.to_datetime(ib_original_dft['ISO_TIME'],format='%Y-%m-%d %H:%M:%S')\n    year_mask = (ib_original_dft['datetime'] > f'{periods[0]}-1-1') & (ib_original_dft['datetime'] <= f'{periods[1]}-11-30')\n    ib_new_period = ib_original_dft[year_mask][ib_original_dft['BASIN'] == 'NA']\n\n    #Only use cyclones over the North Atlantic basin\n    #This can be changed to include more cyclones outside of the Northeast Atlantic\n    def only_na_basin(df):\n        lon_wise = df.sort_values(by='datetime')\n        if lon_wise['LON'].iloc[0] > -55:\n            return df\n        else:\n            return None\n\n    #Get the number of time steps in each cyclone, or event\n    only_neatlantic = ib_new_period.groupby('SID').apply(only_na_basin).reset_index(drop=True)\n    counts = only_neatlantic.groupby('SID').count().iloc[:,0]\n    \n    #Get cyclones that last at least 12 time steps\n    counts_12 = counts[counts > 12].index\n    persist_storms = ib_new_period[ib_new_period['SID'].isin(counts_12)]\n    persist_storms['month']= persist_storms['datetime'].dt.month\n    \n    # mask out land points\n    def mask_lands(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat = ordered_df['LAT']\n        lon = ordered_df['LON']\n        ocean_mask = pd.Series(globe.is_ocean(lat=lat,lon=lon))\n        idx_false = ocean_mask.idxmin()\n        if idx_false == 0:\n            return df\n        else:\n            land_mask = ocean_mask.iloc[:idx_false]\n            final_masked = ordered_df.iloc[:idx_false,:]\n            return final_masked\n   \n    # filter extratropical parts of the storm tracks\n    def filter_ET(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat_filter = ordered_df['LAT'] <= 35\n        filter_df = ordered_df[lat_filter]\n        return filter_df\n    \n    exclude_et = persist_storms.groupby('SID').apply(filter_ET).reset_index(drop=True)\n\n    final_dft = exclude_et.groupby('SID').apply(mask_lands).reset_index(drop=True)\n\n    return final_dft\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":11},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":12},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"content":"\n\nib_data_processed = process_ibrack(ib_data,periods=[2000,2005])\nib_data_processed['LAT'] = ib_data_processed['LAT'].astype(float)\nib_data_processed['LON'] = ib_data_processed['LON'].astype(float)\nib_data_processed['USA_WIND'] = ib_data_processed['USA_WIND'].astype(float)\nib_data_processed['datetime'] = pd.to_datetime(ib_data_processed['datetime'],format='%Y-%m-%d %H:%M:%S')\nib_data_processed['SID'] = ib_data_processed['SID'].astype(str)\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":13},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":14},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"content":"\n\nHint\n\nUp until now, we have primarily been grouping the IBTRACKS data using SID. Starting in this block, we will begin grouping the IBTRACKS data by id. This is done since SID may erroneously connect some cyclone tracks at adjacent time steps, while id appears to keep them separate.\n\nib_data_processed['id'] = ib_data_processed['SID'].astype('category')\nib_data_processed['id'] = ib_data_processed['id'].cat.codes\nreq_cols = ['datetime','LAT','LON','USA_WIND','id']\n\n# groupby datetime 6h \nib_data_processed['datetime'] = ib_data_processed['datetime'].dt.floor('6h')\n\nib_data_processed_6h = ib_data_processed[req_cols].groupby('datetime').mean().reset_index()\n\n# copy the SID based on the id\n\nib_data_processed_6h['SID'] = ib_data_processed.groupby('datetime')['SID'].first().values\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":15},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":16},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"content":"\n\nib_data_processed_6h\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":17},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":18},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"content":"\n\nNote\n\nYou may notice some instances in which the cyclone track hits the top of the boundary, and then appears to erroneously move horizontally over a long distance. Not to worry! This occurs since we discount all cyclones that move north of our boundary. Thus, the plot below will connect the two cyclone positions before and after it leaves the domain. The data is however correct.\n\ndef plot_tracks(filtered_ib):\n    filtered_ib['datetime'] = pd.to_datetime(filtered_ib['datetime'])\n\n    events = filtered_ib.groupby('id')\n    \n    fig,ax = plt.subplots(figsize=(20,10),subplot_kw={\"projection\": ccrs.PlateCarree()})\n    \n    ax.add_feature(cfeature.COASTLINE)\n\n    ax.add_feature(cfeature.BORDERS)\n\n\n    ax.coastlines()\n    \n    ax.add_feature(cfeature.LAND)\n    ax.add_feature(cfeature.OCEAN)\n    \n    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n  \n    for event_num , event in events:\n        lon = event['LON'].values\n        lat = event['LAT'].values\n\n        vertices = [(lo, la) for lo, la in zip(lon, lat)]\n        codes = [Path.MOVETO]\n        [codes.append(Path.LINETO) for index in range(0, len(event) - 1)]\n\n        path = Path(vertices, codes)\n        \n        patch = patches.PathPatch(path, lw = 1, fc = 'none', path_effects = [patheffects.withStroke(linewidth=2.5, foreground=\"black\")], zorder = 5)\n\n        ax.add_patch(patch)\n        \n        ax.set_xlim(-100, 0)\n        ax.set_ylim(5, 35)\n    return fig,ax\n\nplot_tracks(ib_data_processed_6h)\n\nib_data_processed_6h.to_csv('../test_folder/ib_data_processed_6h.csv')\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":19},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#summary","position":20},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"content":"Here, we appropriately edited the cyclone track data over our space and time domain of interest. We have also created an output file that will be useful for eventual creation and training of our machine learning model.","type":"content","url":"/notebooks/ibtrack-preprocessing#summary","position":21},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/ibtrack-preprocessing#whats-next","position":22},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Next we will edit the ERA5 data. In particular, we will take our variables of interest that will be used to train the AI model and organize them for ingestion into the machine learning model.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#whats-next","position":23},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":24},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"content":"Image link: \n\nhttps://​smartcorp​.com​/blog​/what​-silicon​-valley​-bank​-can​-learn​-from​-supply​-chain​-planning​/attachment​/2​-scenarios​-used​-by​-the​-national​-weather​-service​-to​-predict​-hurricane​-tracks/","type":"content","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":25}]}