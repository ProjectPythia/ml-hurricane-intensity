{"version":"1","records":[{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nSee the \n\nCookbook Contributor’s Guide for step-by-step instructions on how to create your new Cookbook and get it hosted on the \n\nPythia Cookbook Gallery!\n\nThis Project Pythia Cookbook covers how to employ a simple AI model to predict hurricane intensity using IBTRACKS and ERA5 environmental variables.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Motivation"},"content":"Here, you will learn how to run a simple AI model to predict hurricane intensity. You will first preprocess ERA5 environmental variables associated with each Tropical Storm in the IBTRACKS dataset. Then, you will use this preprocessed data to create and train an AI model to predict future cyclone intensity. You will gain experience working primarily with xarray and ...","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Authors"},"content":"Nirmal Alex, \n\nMatthew Lynne, etc. Acknowledge primary content authors here","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Structure"},"content":"","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"type":"lvl3","url":"/#introduction","position":10},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Introduction","lvl2":"Structure"},"content":"Read in the data that will be used, including ERA5 and IBTRACKS.","type":"content","url":"/#introduction","position":11},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"type":"lvl3","url":"/#data-preprocessing","position":12},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Data Preprocessing","lvl2":"Structure"},"content":"Process the data into a format that is appropriate for the AI model.","type":"content","url":"/#data-preprocessing","position":13},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"type":"lvl3","url":"/#training-the-model","position":14},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Training the Model","lvl2":"Structure"},"content":"","type":"content","url":"/#training-the-model","position":15},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"Machine Learning Hurricane Intensity Predictions Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\n(Replace “cookbook-example” with the title of your cookbooks)\n\nClone the https://github.com/ProjectPythia/cookbook-example repository: git clone https://github.com/ProjectPythia/cookbook-example.git\n\nMove into the cookbook-example directorycd cookbook-example\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"type":"lvl1","url":"/notebooks/era5-preprocessing","position":0},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing"},"content":"\n\n","type":"content","url":"/notebooks/era5-preprocessing","position":1},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/era5-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Overview"},"content":"Here, we will use the processed IBTRACKS data to select ERA5 environmental variables associated with each cyclone.\n\n","type":"content","url":"/notebooks/era5-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/era5-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/era5-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\nimport numpy as np\nimport dask\n\n","type":"content","url":"/notebooks/era5-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":8},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit and pad ERA5 data"},"content":"\n\nIn this section, we will select ERA5 data within a 5x5 latitude/longitude grid centered at each cyclone center at each time step in our dataset. We will then have to pad the data to account for instances in which grid cells occur over land.\n\ninput_dsets = xr.open_dataset('../test_folder/final_proc_5yr_6h.nc')\n\nib_data_processed_6h = pd.read_csv('../test_folder/ib_data_processed_6h.csv')\n\nfinal_data = []\nmax_len = ib_data_processed_6h.groupby('id').size().max()  # assuming max length is 3 hours per storm\n\nfor id_number,group in ib_data_processed_6h.groupby('id'):\n    events_data = []\n    for index,row in group.iterrows():\n        lat = int(row['LAT'])\n        lon = int(row['LON'])\n        time = row['datetime']\n        \n        #We want data in a 5x5 latitude/longitude grid centered on the cyclone latitude/longitude\n        latmin = lat - 2\n        latmax = lat + 2\n        lonmin = lon - 2\n        lonmax = lon + 2\n        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n        \n            \n        final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n        final_xr['x'] = np.arange(0,final_xr.sizes['x'])\n        final_xr['y'] = np.arange(0,final_xr.sizes['y'])\n        \n        # fill NaN values with zeros along the x and y dimensions\n        for jj in final_xr.data_vars:\n            final_xr[jj].fillna(0)  # Fill NaN values\n        \n        #Recall that we are trying to predict the wind speed.\n        #Hence, our target is USA_WIND\n        final_xr['target'] = row['USA_WIND']    \n        events_data.append(final_xr)\n    \n    final_event = xr.concat(events_data,dim='time')\n    id\n    #Pad data with zeros up to the maximum time\n    if len(final_event.time) <= max_len:\n        new_time = pd.date_range(start=final_event['time'].min().values, periods=max_len ,freq='6h')\n        padded_data = final_event.reindex(time=new_time, fill_value=0.0)\n    else:\n        padded_data = final_event\n    \n    lead_time = np.arange(0,max_len*6 ,6)\n    padded_data['lead'] = ('time', lead_time)\n    padded_data = padded_data.assign_coords({'lead': padded_data['lead'].astype(int)})\n    \n    # swap time and lead dimensions\n    padded_data = padded_data.swap_dims({'time': 'lead'})\n    padded_data['id'] = id_number \n    \n    # convert the time dimension to a variable\n    final_data.append(padded_data)\n\nfinal_input_padded = xr.concat(final_data, dim='SID')\nfinal_input_padded\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-and-pad-era5-data","position":9},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictands for each cyclone"},"type":"lvl2","url":"/notebooks/era5-preprocessing#edit-predictands-for-each-cyclone","position":10},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Edit predictands for each cyclone"},"content":"Here we can move on to our second objective, to explicitly edit the predictands that will be used by the machine learning model. The predictands are the environmental variables associated with a cyclone at each time step. Some of these variables include sea surface temperatures and lower tropospheric humidity.\n\ndef process_row(row, input_dsets):\n    lat = int(row['LAT'])\n    lon = int(row['LON'])\n    time = row['datetime']\n    latmin = lat - 5\n    latmax = lat + 5\n    lonmin = lon - 5\n    lonmax = lon + 5\n\n    try:\n        # Select the data for the given lat/lon/time\n        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n    \n    except KeyError:\n        # If data is not found, return None (will be filtered out later)\n        print(f\"Data not found for SID: {row['SID']} at time {time} with lat {lat} and lon {lon}\")\n        return None\n\n    # Add SID and wind speed as new variables\n    sel_data['id'] = row['id']\n    wind_speed = row['USA_WIND']\n\n    # Rename dimensions and set coordinate ranges\n    final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n    final_xr['x'] = np.arange(0, len(final_xr['x']), 1)\n    final_xr['y'] = np.arange(0, len(final_xr['y']), 1)\n    final_xr = final_xr.fillna(0)  # Fill NaN values with zeros\n    final_xr['target'] = wind_speed\n\n    return final_xr\n\n# Wrap your row processing in dask.delayed\ndelayed_results = []\nfor index, row in ib_data_processed.iterrows():\n    delayed_result = dask.delayed(process_row)(row, input_dsets)\n    delayed_results.append(delayed_result)\n\n# Compute in parallel and filter out None results\nfinal_data = dask.compute(*delayed_results)\nfinal_data = [ds for ds in final_data if ds is not None]\n\n# Concatenate along 'time' dimension\nfinal_data_xr = xr.concat(final_data, dim='time')\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nMyST Syntax Overview for MyST-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/era5-preprocessing#edit-predictands-for-each-cyclone","position":11},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/era5-preprocessing#last-section","position":12},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Last Section"},"content":"You can add \n\nadmonitions using MyST syntax:\n\nNote\n\nYour relevant information here!\n\nSome other admonitions you can put in (\n\nthere are 10 total):\n\nHint\n\nA helpful hint.\n\nWarning\n\nBe careful!\n\nDanger\n\nScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/era5-preprocessing#last-section","position":13},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/era5-preprocessing#summary","position":14},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/era5-preprocessing#summary","position":15},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/era5-preprocessing#whats-next","position":16},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/era5-preprocessing#whats-next","position":17},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/era5-preprocessing#resources-and-references","position":18},{"hierarchy":{"lvl1":"ERA5 Data Preprocessing","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/era5-preprocessing#resources-and-references","position":19},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"type":"lvl1","url":"/notebooks/ibtrack-preprocessing","position":0},{"hierarchy":{"lvl1":"Preprocess IBTRACK data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing","position":1},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#overview","position":2},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Overview"},"content":"The first step of any machine learning algorithm is to first load in, filter, and then process the necessary data. Here, we will load in the hurricane track data from IBTRACKS and then filter the tracks by requirements that we will set.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#overview","position":3},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to NUMPY\n\nNecessary\n\n\n\nIntro to PANDAS\n\nNecessary\n\n\n\nIntro to XARRAY\n\nNecessary\n\n\n\nProject management\n\nHelpful\n\n\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport xarray as xr \nfrom dask.distributed import Client\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport pandas as pd\nimport glob\nfrom global_land_mask import globe\nimport cartopy.feature as cfeature\nfrom matplotlib.path import Path\nimport matplotlib.patches as patches\nfrom matplotlib import patheffects\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":8},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Read in IBTRACKS data"},"content":"\n\nFirst, read in the data as a csv file\n\nib_data = '~/ml-hurricane-intensity/test_folder/ibtracs.NA.list.v04r00.csv'\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#read-in-ibtracks-data","position":9},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":10},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Method to preprocess the track data"},"content":"The following method will set custom ranges for the time period and spatial domain of analysis.\n\ndef process_ibrack(ib_loc, periods=[2000,2005]):\n\n    #Read in the IBTRACKS data\n    read_ib_data = pd.read_csv(ib_loc,keep_default_na=False)\n    \n    #Get the units for each column in read_ib_data\n    units = read_ib_data.iloc[0,:]\n\n    #Get data or the remainder of read_ib_data\n    ib_original_dft = read_ib_data.iloc[1:,:]\n\n    #Set a custom date and time range based on user choosing\n    ib_original_dft['datetime'] = pd.to_datetime(ib_original_dft['ISO_TIME'],format='%Y-%m-%d %H:%M:%S')\n    year_mask = (ib_original_dft['datetime'] > f'{periods[0]}-1-1') & (ib_original_dft['datetime'] <= f'{periods[1]}-11-30')\n    ib_new_period = ib_original_dft[year_mask][ib_original_dft['BASIN'] == 'NA']\n\n    #Only use cyclones over the North Atlantic basin\n    #This can be changed to include more cyclones outside of the Northeast Atlantic\n    def only_na_basin(df):\n        lon_wise = df.sort_values(by='datetime')\n        if lon_wise['LON'].iloc[0] > -55:\n            return df\n        else:\n            return None\n\n    #Get the number of time steps in each cyclone, or event\n    only_neatlantic = ib_new_period.groupby('SID').apply(only_na_basin).reset_index(drop=True)\n    counts = only_neatlantic.groupby('SID').count().iloc[:,0]\n    \n    #Get cyclones that last at least 12 time steps\n    counts_12 = counts[counts > 12].index\n    persist_storms = ib_new_period[ib_new_period['SID'].isin(counts_12)]\n    persist_storms['month']= persist_storms['datetime'].dt.month\n    \n    # mask out land points\n    def mask_lands(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat = ordered_df['LAT']\n        lon = ordered_df['LON']\n        ocean_mask = pd.Series(globe.is_ocean(lat=lat,lon=lon))\n        idx_false = ocean_mask.idxmin()\n        if idx_false == 0:\n            return df\n        else:\n            land_mask = ocean_mask.iloc[:idx_false]\n            final_masked = ordered_df.iloc[:idx_false,:]\n            return final_masked\n   \n    # filter extratropical parts of the storm tracks\n    def filter_ET(df):\n        ordered_df = df.sort_values(by='datetime')\n        lat_filter = ordered_df['LAT'] <= 35\n        filter_df = ordered_df[lat_filter]\n        return filter_df\n    \n    exclude_et = persist_storms.groupby('SID').apply(filter_ET).reset_index(drop=True)\n\n    final_dft = exclude_et.groupby('SID').apply(mask_lands).reset_index(drop=True)\n\n    return final_dft\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#method-to-preprocess-the-track-data","position":11},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":12},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Process IBTRACKS data using the above methods"},"content":"\n\nib_data_processed = process_ibrack(ib_data,periods=[2000,2005])\nib_data_processed['LAT'] = ib_data_processed['LAT'].astype(float)\nib_data_processed['LON'] = ib_data_processed['LON'].astype(float)\nib_data_processed['USA_WIND'] = ib_data_processed['USA_WIND'].astype(float)\nib_data_processed['datetime'] = pd.to_datetime(ib_data_processed['datetime'],format='%Y-%m-%d %H:%M:%S')\nib_data_processed['SID'] = ib_data_processed['SID'].astype(str)\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#process-ibtracks-data-using-the-above-methods","position":13},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":14},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Organize the edited IBTRACKS file"},"content":"\n\nHint\n\nUp until now, we have primarily been grouping the IBTRACKS data using SID. Starting in this block, we will begin grouping the IBTRACKS data by id. This is done since SID may erroneously connect some cyclone tracks at adjacent time steps, while id appears to keep them separate.\n\nib_data_processed['id'] = ib_data_processed['SID'].astype('category')\nib_data_processed['id'] = ib_data_processed['id'].cat.codes\nreq_cols = ['datetime','LAT','LON','USA_WIND','id']\n\n# groupby datetime 6h \nib_data_processed['datetime'] = ib_data_processed['datetime'].dt.floor('6h')\n\nib_data_processed_6h = ib_data_processed[req_cols].groupby('datetime').mean().reset_index()\n\n# copy the SID based on the id\n\nib_data_processed_6h['SID'] = ib_data_processed.groupby('datetime')['SID'].first().values\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#organize-the-edited-ibtracks-file","position":15},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":16},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"See the organized and edited IBTRACKS file"},"content":"\n\nib_data_processed_6h\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#see-the-organized-and-edited-ibtracks-file","position":17},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":18},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Plot the track data we have edited and organized"},"content":"\n\nNote\n\nYou may notice some instances in which the cyclone track hits the top of the boundary, and then appears to erroneously move horizontally over a long distance. Not to worry! This occurs since we discount all cyclones that move north of our boundary. Thus, the plot below will connect the two cyclone positions before and after it leaves the domain. The data is however correct.\n\ndef plot_tracks(filtered_ib):\n    filtered_ib['datetime'] = pd.to_datetime(filtered_ib['datetime'])\n\n    events = filtered_ib.groupby('id')\n    \n    fig,ax = plt.subplots(figsize=(20,10),subplot_kw={\"projection\": ccrs.PlateCarree()})\n    \n    ax.add_feature(cfeature.COASTLINE)\n\n    ax.add_feature(cfeature.BORDERS)\n\n\n    ax.coastlines()\n    \n    ax.add_feature(cfeature.LAND)\n    ax.add_feature(cfeature.OCEAN)\n    \n    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n  \n    for event_num , event in events:\n        lon = event['LON'].values\n        lat = event['LAT'].values\n\n        vertices = [(lo, la) for lo, la in zip(lon, lat)]\n        codes = [Path.MOVETO]\n        [codes.append(Path.LINETO) for index in range(0, len(event) - 1)]\n\n        path = Path(vertices, codes)\n        \n        patch = patches.PathPatch(path, lw = 1, fc = 'none', path_effects = [patheffects.withStroke(linewidth=2.5, foreground=\"black\")], zorder = 5)\n\n        ax.add_patch(patch)\n        \n        ax.set_xlim(-100, 0)\n        ax.set_ylim(5, 35)\n    return fig,ax\n\nplot_tracks(ib_data_processed_6h)\n\nib_data_processed_6h.to_csv('../test_folder/ib_data_processed_6h.csv')\n\n\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#plot-the-track-data-we-have-edited-and-organized","position":19},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#summary","position":20},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Summary"},"content":"Here, we appropriately edited the cyclone track data over our space and time domain of interest. We have also created an output file that will be useful for eventual creation and training of our machine learning model.","type":"content","url":"/notebooks/ibtrack-preprocessing#summary","position":21},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/ibtrack-preprocessing#whats-next","position":22},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Next we will edit the ERA5 data. In particular, we will take our variables of interest that will be used to train the AI model and organize them for ingestion into the machine learning model.\n\n","type":"content","url":"/notebooks/ibtrack-preprocessing#whats-next","position":23},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":24},{"hierarchy":{"lvl1":"Preprocess IBTRACK data","lvl2":"Resources and references"},"content":"Image link: \n\nhttps://​smartcorp​.com​/blog​/what​-silicon​-valley​-bank​-can​-learn​-from​-supply​-chain​-planning​/attachment​/2​-scenarios​-used​-by​-the​-national​-weather​-service​-to​-predict​-hurricane​-tracks/","type":"content","url":"/notebooks/ibtrack-preprocessing#resources-and-references","position":25}]}