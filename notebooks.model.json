{"version":2,"kind":"Notebook","sha256":"10c6ac35e1c251e1748c59341e7cae0a109e3773818bc009d612630525265fe4","slug":"notebooks.model","location":"/notebooks/Model.ipynb","dependencies":[],"frontmatter":{"title":"Model Setup","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"nma","language":"python"},"authors":[{"nameParsed":{"literal":"Nirmal Alex, Matthew Lynne","given":"Matthew Lynne","family":"Nirmal Alex"},"name":"Nirmal Alex, Matthew Lynne","id":"contributors-myst-generated-uid-0"}],"open_access":true,"license":{"content":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/","name":"Creative Commons Attribution 4.0 International","free":true,"CC":true},"code":{"id":"Apache-2.0","url":"https://opensource.org/licenses/Apache-2.0","name":"Apache License 2.0","free":true,"osi":true}},"github":"https://github.com/ProjectPythia/ml-hurricane-intensity","copyright":"2024","affiliations":[{"id":"UAlbany","name":"University at Albany (SUNY)","department":"Atmospheric and Environmental Sciences","url":"https://www.albany.edu/daes"},{"id":"CISL","name":"NSF National Center for Atmospheric Research","department":"Computational and Information Systems Lab","url":"https://www.cisl.ucar.edu"},{"id":"Unidata","name":"NSF Unidata","url":"https://www.unidata.ucar.edu"},{"id":"Argonne","name":"Argonne National Laboratory","department":"Environmental Science Division","url":"https://www.anl.gov/evs"},{"id":"CarbonPlan","name":"CarbonPlan","url":"https://carbonplan.org"},{"id":"NVIDIA","name":"NVIDIA Corporation","url":"https://www.nvidia.com/"}],"numbering":{"title":{"offset":1}},"edit_url":"https://github.com/ProjectPythia/ml-hurricane-intensity/blob/main/notebooks/Model.ipynb","exports":[{"format":"ipynb","filename":"Model.ipynb","url":"/ml-hurricane-intensity/build/Model-a676baa6482ac10d5574663ec1c88f6c.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"thematicBreak","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"c0AUJnw0bB"}],"key":"zBAoZJCAcV"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Overview","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"votd9cSqqy"}],"identifier":"overview","label":"Overview","html_id":"overview","implicit":true,"key":"gCfjeHwLWq"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"This notebook is for setting up a simple CNN-LSTM model to predict event-wise hurricane intensity using the prreproceesed data from the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Ue5yqvKrwy"},{"type":"inlineCode","value":"era5_preprocessing.ipynb","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"pIINcEJOJf"},{"type":"text","value":" notebook.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"WNuaTxSH9l"}],"key":"URGtuzIUGj"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"strong","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Data Preparation","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"D821mcpaAx"}],"key":"HyvxUHGrUY"},{"type":"text","value":": Load and preprocess the data.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"iZss4SL3ZG"}],"key":"C0jL6TpqC9"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Model Definition","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"qwX2UOmTRk"}],"key":"L8w0JmxUA5"},{"type":"text","value":": Define a CNN-LSTM model.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"SsqlxBDO5D"}],"key":"rLlpNkBBvj"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Model Training","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Ro4OWFjSt2"}],"key":"JjlJZbREpq"},{"type":"text","value":": Train the model on the prepared data.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"K74JTagGKF"}],"key":"gSY4QGOJhQ"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Model Evaluation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"jTyAkizmS0"}],"key":"ooiCOMrviO"},{"type":"text","value":": Evaluate the modelâ€™s performance on test data.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"ybKahc8DCa"}],"key":"r063OShRcI"}],"key":"tokmTVOlf9"}],"key":"GvEPtiVK44"},{"type":"block","kind":"notebook-content","children":[{"type":"thematicBreak","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GJvlesDwkO"}],"key":"PQbq6OzsUP"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Imports","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IazOSBTox5"}],"identifier":"imports","label":"Imports","html_id":"imports","implicit":true,"key":"FuNB6xvsxE"}],"key":"AFY8lnTcJE"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport xarray as xr\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras.layers import  TimeDistributed, LSTM\nimport visualkeras\nimport tensorflow as tf\n","key":"WWyO9IAjXM"},{"type":"output","id":"xWH6kyDDqq7vX0FvLVimP","data":[],"key":"PYg1xDTzNW"}],"key":"GugeStXb3i"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Load the hurricane wise variable dataset","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bmIoluh2yu"}],"identifier":"load-the-hurricane-wise-variable-dataset","label":"Load the hurricane wise variable dataset","html_id":"load-the-hurricane-wise-variable-dataset","implicit":true,"key":"Xo4RPz7xSA"}],"key":"qLXvdjEidt"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In this section,we will load the preprocessed dataset containing hurricane-wise environmental variables. This dataset is essential for training our CNN-LSTM model to predict hurricane intensity.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WrGgtFrpUg"}],"key":"ijiucYlyFG"}],"key":"kYF7v49YZX"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# load the preprocessed dataset\nmodel_input = xr.open_dataset('../test_folder/input_predictands.nc')\nmodel_input\n","key":"USaN9K6UeP"},{"type":"output","id":"5tOsUU8uDrM0xbP2W3om7","data":[{"output_type":"error","traceback":"\u001b[31m---------------------------------------------------------------------------\u001b[39m\n\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load the preprocessed dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_input = \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../test_folder/input_predictands.nc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m model_input\n\n\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/cookbook-dev/lib/python3.12/site-packages/xarray/backends/api.py:696\u001b[39m, in \u001b[36mopen_dataset\u001b[39m\u001b[34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, create_default_indexes, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[39m\n\u001b[32m    693\u001b[39m     kwargs.update(backend_kwargs)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     engine = \u001b[43mplugins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     from_array_kwargs = {}\n\n\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/cookbook-dev/lib/python3.12/site-packages/xarray/backends/plugins.py:194\u001b[39m, in \u001b[36mguess_engine\u001b[39m\u001b[34m(store_spec)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     error_msg = (\n\u001b[32m    188\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfound the following matches with the input file in xarray\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms IO \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\n\u001b[31mValueError\u001b[39m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html","ename":"ValueError","evalue":"did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"}],"key":"M0U9GnvzqY"}],"key":"Jq7oYxRaVp"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Input Data prerpocessing steps","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Cw8XCEZWTZ"}],"identifier":"input-data-prerpocessing-steps","label":"Input Data prerpocessing steps","html_id":"input-data-prerpocessing-steps","implicit":true,"key":"KCS6v3O14Q"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Nan and padded values will be set to zero","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"FgCSVWi28y"}],"key":"a1bGGvdwm1"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Set the train and test split","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"achgopH22j"}],"key":"JBnk04IvuV"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"normalize using the MinMaxScaler","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"jVWmdA3pTT"}],"key":"osGJRgGBvV"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"random shuffle for generalization","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RqMJEM3LwA"}],"key":"t1QERMGYV9"}],"key":"TgzLZ1h14C"}],"key":"xMsGbJhZ9e"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# remove the nan values and set it \nmodel_input = model_input.fillna(0)\n\n# selecting the predictors (X) and expanding the dimensions\n\nX_data = model_input[['u','v','vo','speed_shear','sp','r','cor_params']].to_array(dim='variable')\n\nprint(f'Dimensions are , features: {X_data.shape[0]}, Event: {X_data.shape[1]}, time(lead): {X_data.shape[2]}, lat: {X_data.shape[3]}, lon: {X_data.shape[4]}')\n\nX_data = X_data.transpose('id', 'lead', 'y','x','variable')\n\nprint(f'X_data dimensions are: Event: {X_data.shape[0]}, time(lead): {X_data.shape[1]}, lat: {X_data.shape[2]}, lon: {X_data.shape[3]}, features: {X_data.shape[4]}')","key":"dXUXLs1TTJ"},{"type":"output","id":"Rl8i-j0Oa39uvPfH8NrUw","data":[],"key":"jK2tlG9JIT"}],"key":"wn7yrztTSd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# selecting the target variable (y)\nY_data = model_input['target']\n\n# expanded the dimensions of Y_data to match the expected input shape for the model\n#Y_data = np.expand_dims(Y_data, axis=-1)\n\nprint(f'Target dimensions are: Event: {Y_data.shape[0]}, time(lead): {Y_data.shape[1]}')","key":"tHDikWStUO"},{"type":"output","id":"Ina9K9WhUh9Zdi3iHOb7e","data":[],"key":"dnqNVomfnj"}],"key":"b9T6WtR1eq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"## 80% train and 20% test split\n\n## random shuffled the events and split the data into training and testing sets\nX_train , X_test , Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=1)\n\n\nX_train_data = X_train.values\nX_test_data = X_test.values\nY_train_data = Y_train.values\nY_test_data = Y_test.values\n\n\n","key":"m5YOdq2wsp"},{"type":"output","id":"TLCz6KzHUyvaC_yxXuZPU","data":[],"key":"UfiB44SAIh"}],"key":"rgPd5XQoRP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"x_train_scaler = MinMaxScaler()\nx_test_scaler = MinMaxScaler()\n\ny_train_scaler = MinMaxScaler()\ny_test_scaler = MinMaxScaler()","key":"q0PTg4tVPH"},{"type":"output","id":"KoyoL4r2wb1QkWHMCNE11","data":[],"key":"IQA4UpJnLn"}],"key":"ao5vXdBIiL"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"X_train_scaled = x_train_scaler.fit_transform(X_train_data.reshape(-1, X_train_data.shape[-1])).reshape(X_train_data.shape)\nY_train_scaled = y_train_scaler.fit_transform(Y_train_data.reshape(-1,1)).reshape(Y_train_data.shape)\n\nX_test_scaled = x_test_scaler.fit_transform(X_test_data.reshape(-1, X_test_data.shape[-1])).reshape(X_test_data.shape)\nY_test_scaled = y_test_scaler.fit_transform(Y_test_data.reshape(-1,1)).reshape(Y_test_data.shape)\n","key":"V5fmA2ZyWN"},{"type":"output","id":"aYZFDVlQWqm_ClRCxcB6X","data":[],"key":"hpCjQ4HKD7"}],"key":"JY1eyPJiOH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def masked_mse(y_true, y_pred):\n    mask = tf.cast(tf.not_equal(y_true, 0.0), tf.float32)\n    squared_error = tf.square(y_true - y_pred)\n    masked_loss = tf.reduce_sum(squared_error * mask) / (tf.reduce_sum(mask) + 1e-6)\n    return masked_loss\n\nmodel = Sequential()\nmodel.add(TimeDistributed(\n    Conv2D(16, (3, 3), activation='relu', padding='same'),\n    input_shape=(140, 5, 5, 7)\n))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(64, return_sequences=True))  # <--- important!\nmodel.add(TimeDistributed(Dense(1)))\n\nmodel.summary()\n","key":"nL9rIKHuHU"},{"type":"output","id":"Kw9w6FW5pgLEaAPuBESNR","data":[],"key":"wqPG3K37HY"}],"key":"D5yxkkihOi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualkeras.layered_view(model,scale_xy=0.6)\n","key":"nXSss0Uzdg"},{"type":"output","id":"1EHUk51ooaZpzGD2MfI0B","data":[],"key":"BQAzYucjN2"}],"key":"I0DMjAKsd8"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"model.compile(optimizer='adam', metrics=['mae'] , loss=masked_mse)\n\nmodel.fit(X_train_scaled, Y_train_scaled, epochs=100, batch_size=32, validation_split=0.2)","key":"wK0HPPB8pD"},{"type":"output","id":"sLeA-tYrGHLHdtOf2UZGr","data":[],"key":"r1JslrksbJ"}],"key":"bwjKXOi2M9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"predict_x = model.predict(X_test_scaled)","key":"QiJMmNJ8Hv"},{"type":"output","id":"cJR4G-KwSq_uo6aoLLsjA","data":[],"key":"U1GjLIqNhw"}],"key":"Yk0k81eRut"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def invert_add_meta(data,scalar,xr_data):\n    data = scalar.inverse_transform(data.reshape(-1 ,1)).reshape(xr_data.shape)\n    added_meta = xr.DataArray(data, coords=xr_data.coords, dims=xr_data.dims)\n    return added_meta\n\npredicted_wind_speed = invert_add_meta(predict_x, y_test_scaler, Y_test)\n\nobserved_wind_speeds = Y_test\n\nfinal_dset = predicted_wind_speed.to_dataset(name='predicted_wind_speed')\nfinal_dset['observed_wind_speed'] = observed_wind_speeds\n                                                    \nfinal_dft = final_dset.to_dataframe().reset_index()\n\n# set 0.0 as nan in observed_wind_speed\nfinal_dft['observed_wind_speed'] = final_dft['observed_wind_speed'].replace(0.0, np.nan)\n\n# whenever the predicted_wind_speed is nan, sdrop the entire row\n\nfinal_dft = final_dft.dropna(how='any',axis=0)\n\nfinal_pivot_col = final_dft.drop(columns=['time','level'])","key":"oSuDTa7duf"},{"type":"output","id":"ifIXzefb4IXiyW1yCaNtA","data":[],"key":"CAbYcUOYJr"}],"key":"T66hldgSBH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"final_pivot_col","key":"LSAXTsnDUv"},{"type":"output","id":"yziWUv-_sLZxMR2lOXSiE","data":[],"key":"Rh74sn7fuZ"}],"key":"vHLzbZRziT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import seaborn as sbs \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define two different palettes\npalette1 = sns.color_palette(\"Reds\")           # for predicted\npalette2 = sns.color_palette(\"Blues\")            # for observed\n\nfig,ax = plt.subplots(figsize=(12, 6))\n# First lineplot (predicted) with palette1\nsns.lineplot(\n    data=final_pivot_col,\n    x='lead',\n    y='predicted_wind_speed',\n    hue='id',\n    style='id',\n    markers=True,\n    dashes=False,\n    palette=palette1,\n    legend='brief'\n,ax=ax)\n\n# Second lineplot (observed) with palette2\nsns.lineplot(\n    data=final_pivot_col,\n    x='lead',\n    y='observed_wind_speed',\n    hue='id',\n    style='id',\n    markers=True,\n    dashes=False,\n    palette=palette2,\n    legend='brief'\n,ax=ax)\nplt.legend('')\n\n\n\n# add H to the end of xticks\nxticks = ax.get_xticks()\nax.set_xticks(xticks)\nax.set_xticklabels([f'{int(tick)}H' for tick in xticks])\nax.set_xlim(0, 500)\n\n\nax.set_xlabel('Lead (Hours)',fontsize=15)\nax.set_title('Predicted (Reds) vs Observed Wind Speed (Blues)',fontsize=18)\n\nax.set_ylabel('Wind Speed (m/s)',fontsize=15)\n#sbs.lineplot(data=final_pivot_col, x='lead', y='observed_wind_speed', hue='id', style='id', markers=True, dashes=False)","key":"mdpyUOrfdx"},{"type":"output","id":"lfudDtRwtQHflm6buCsav","data":[],"key":"HNVnNg331q"}],"key":"dDzWlyo6NA"}],"key":"Rsm6Ary3Mf"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"ERA5 Data Preprocessing","url":"/notebooks/era5-preprocessing","group":"Data Preprocessing"}}},"domain":"http://localhost:3000"}