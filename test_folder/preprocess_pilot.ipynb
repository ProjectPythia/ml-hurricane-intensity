{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfcadd-ace1-4995-88dc-053f83224a76",
   "metadata": {},
   "source": [
    "## Start here\n",
    "# Read in ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778182ef-2186-4d46-bfce-0fc0a47fdd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8afcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dsets = xr.open_dataset('~/Data/final_proc_5yr_6h.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating coriolis parameter \n",
    "import numpy as np\n",
    "\n",
    "cor_parms =  2 * 7.29 * 1e-5 * np.sin(np.radians(input_dsets['latitude']))\n",
    "\n",
    "input_dsets['cor_params'] = xr.DataArray(cor_parms,\n",
    "                                            name='cor_params'\n",
    "                                            ).broadcast_like(input_dsets['r'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d016e-cd81-4589-9008-f35f4a0ca9c2",
   "metadata": {},
   "source": [
    "## Read in IBTRACKS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4785db-5335-423b-91a6-bf5adc477f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filepath for IBTRACKS\n",
    "ib_data = '~/ml-hurricane-intensity/test_folder/ibtracs.NA.list.v04r00.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e710e-d34d-4e55-9ef2-1e3aec890956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_land_mask import globe\n",
    "\n",
    "def process_ibrack(ib_loc, periods=[2000,2005]):\n",
    "\n",
    "    #Read in the IBTRACKS data\n",
    "    read_ib_data = pd.read_csv(ib_loc,keep_default_na=False)\n",
    "    \n",
    "    #Get the units for each column in read_ib_data\n",
    "    units = read_ib_data.iloc[0,:]\n",
    "\n",
    "    #Get data or the remainder of read_ib_data\n",
    "    ib_original_dft = read_ib_data.iloc[1:,:]\n",
    "\n",
    "    #Set a custom date and time range based on user choosing\n",
    "    ib_original_dft['datetime'] = pd.to_datetime(ib_original_dft['ISO_TIME'],format='%Y-%m-%d %H:%M:%S')\n",
    "    year_mask = (ib_original_dft['datetime'] > f'{periods[0]}-1-1') & (ib_original_dft['datetime'] <= f'{periods[1]}-11-30')\n",
    "    ib_new_period = ib_original_dft[year_mask][ib_original_dft['BASIN'] == 'NA']\n",
    "\n",
    "    #Only use cyclones over the North Atlantic basin\n",
    "    #This can be changed to include more cyclones outside of the Northeast Atlantic\n",
    "    def only_na_basin(df):\n",
    "        lon_wise = df.sort_values(by='datetime')\n",
    "        if lon_wise['LON'].iloc[0] > -55:\n",
    "            return df\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    #Get the number of time steps in each cyclone, or event\n",
    "    only_neatlantic = ib_new_period.groupby('SID').apply(only_na_basin).reset_index(drop=True)\n",
    "    counts = only_neatlantic.groupby('SID').count().iloc[:,0]\n",
    "    \n",
    "    #Get cyclones that last at least 12 time steps\n",
    "    counts_12 = counts[counts > 12].index\n",
    "    persist_storms = ib_new_period[ib_new_period['SID'].isin(counts_12)]\n",
    "    persist_storms['month']= persist_storms['datetime'].dt.month\n",
    "    \n",
    "    # mask out land points\n",
    "    def mask_lands(df):\n",
    "        ordered_df = df.sort_values(by='datetime')\n",
    "        lat = ordered_df['LAT']\n",
    "        lon = ordered_df['LON']\n",
    "        ocean_mask = pd.Series(globe.is_ocean(lat=lat,lon=lon))\n",
    "        idx_false = ocean_mask.idxmin()\n",
    "        if idx_false == 0:\n",
    "            return df\n",
    "        else:\n",
    "            land_mask = ocean_mask.iloc[:idx_false]\n",
    "            final_masked = ordered_df.iloc[:idx_false,:]\n",
    "            return final_masked\n",
    "   \n",
    "    # filter extratropical parts of the storm tracks\n",
    "    def filter_ET(df):\n",
    "        ordered_df = df.sort_values(by='datetime')\n",
    "        lat_filter = ordered_df['LAT'] <= 35\n",
    "        filter_df = ordered_df[lat_filter]\n",
    "        return filter_df\n",
    "    \n",
    "    exclude_et = persist_storms.groupby('SID').apply(filter_ET).reset_index(drop=True)\n",
    "\n",
    "    final_dft = exclude_et.groupby('SID').apply(mask_lands).reset_index(drop=True)\n",
    "\n",
    "    return final_dft\n",
    "\n",
    "ib_data_processed = process_ibrack(ib_data,periods=[2000,2005])\n",
    "ib_data_processed['LAT'] = ib_data_processed['LAT'].astype(float)\n",
    "ib_data_processed['LON'] = ib_data_processed['LON'].astype(float)\n",
    "ib_data_processed['USA_WIND'] = ib_data_processed['USA_WIND'].astype(float)\n",
    "ib_data_processed['datetime'] = pd.to_datetime(ib_data_processed['datetime'],format='%Y-%m-%d %H:%M:%S')\n",
    "ib_data_processed['SID'] = ib_data_processed['SID'].astype(str)\n",
    "\n",
    "ib_data_processed['id'] = ib_data_processed['SID'].astype('category')\n",
    "ib_data_processed['id'] = ib_data_processed['id'].cat.codes\n",
    "req_cols = ['datetime','LAT','LON','USA_WIND','id']\n",
    "\n",
    "# groupby datetime 6h \n",
    "ib_data_processed['datetime'] = ib_data_processed['datetime'].dt.floor('6h')\n",
    "\n",
    "ib_data_processed_6h = ib_data_processed[req_cols].groupby('datetime').mean().reset_index()\n",
    "\n",
    "# copy the SID based on the id\n",
    "\n",
    "ib_data_processed_6h['SID'] = ib_data_processed.groupby('datetime')['SID'].first().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c863376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessed IBTRACK data\n",
    "ib_data_processed_6h\n",
    "\n",
    "print(ib_data_processed_6h.loc[ib_data_processed_6h['SID'] == \"2000260N15308\"])\n",
    "\n",
    "#for row, col in ib_data_processed_6h.iterrows():\n",
    "#    print(col[5], col[1], col[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e278a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.feature as cfeature\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import patheffects\n",
    "\n",
    "def plot_tracks(filtered_ib):\n",
    "    filtered_ib['datetime'] = pd.to_datetime(filtered_ib['datetime'])\n",
    "\n",
    "    events = filtered_ib.groupby('id')\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(20,10),subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "\n",
    "\n",
    "    ax.coastlines()\n",
    "    \n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    \n",
    "    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "  \n",
    "    for event_num , event in events:\n",
    "        #print(event_num)\n",
    "        #if(event_num == 8.0):   \n",
    "        print(event_num)\n",
    "        #roll_events = event.set_index('datetime')[['LON','LAT']].rolling(window=1).mean()\n",
    "        lon = event['LON'].values\n",
    "        lat = event['LAT'].values\n",
    "\n",
    "        vertices = [(lo, la) for lo, la in zip(lon, lat)]\n",
    "        codes = [Path.MOVETO]\n",
    "        [codes.append(Path.LINETO) for index in range(0, len(event) - 1)]\n",
    "    \n",
    "        print(vertices)\n",
    "        print(codes)\n",
    "\n",
    "        path = Path(vertices, codes)\n",
    "\n",
    "        #print(path)\n",
    "\n",
    "        patch = patches.PathPatch(path, lw = 1, fc = 'none', path_effects = [patheffects.withStroke(linewidth=2.5, foreground=\"black\")], zorder = 5)\n",
    "\n",
    "        ax.add_patch(patch)\n",
    "        \n",
    "        #ax.plot(lon,lat,transform=ccrs.PlateCarree(),linewidth=1,color='black')\n",
    "        ax.set_xlim(-100, 0)\n",
    "        ax.set_ylim(5, 35)\n",
    "    return fig,ax\n",
    "\n",
    "plot_tracks(ib_data_processed_6h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_data_processed_6h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dsets.sel(time='2002-09-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through rows iterrows\n",
    "\n",
    "final_data = []\n",
    "max_len = ib_data_processed_6h.groupby('id').size().max()  # assuming max length is 3 hours per storm\n",
    "for id_number,group in ib_data_processed_6h.groupby('id'):\n",
    "    events_data = []\n",
    "    for index,row in group.iterrows():\n",
    "        lat = int(row['LAT'])\n",
    "        lon = int(row['LON'])\n",
    "        time = row['datetime']\n",
    "        \n",
    "        #We want data in a 5x5 latitude/longitude grid centered on the cyclone latitude/longitude\n",
    "        latmin = lat - 2\n",
    "        latmax = lat + 2\n",
    "        lonmin = lon - 2\n",
    "        lonmax = lon + 2\n",
    "        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n",
    "        \n",
    "            \n",
    "        final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "        final_xr['x'] = np.arange(0,final_xr.sizes['x'])\n",
    "        final_xr['y'] = np.arange(0,final_xr.sizes['y'])\n",
    "        \n",
    "        # fill NaN values with zeros along the x and y dimensions\n",
    "        for jj in final_xr.data_vars:\n",
    "            final_xr[jj].fillna(0)  # Fill NaN values\n",
    "        \n",
    "        #Recall that we are trying to predict the wind speed.\n",
    "        #Hence, our target is USA_WIND\n",
    "        final_xr['target'] = row['USA_WIND']    \n",
    "        events_data.append(final_xr)\n",
    "    \n",
    "    final_event = xr.concat(events_data,dim='time')\n",
    "    id\n",
    "    #Pad data with zeros up to the maximum time\n",
    "    if len(final_event.time) <= max_len:\n",
    "        new_time = pd.date_range(start=final_event['time'].min().values, periods=max_len ,freq='6h')\n",
    "        padded_data = final_event.reindex(time=new_time, fill_value=0.0)\n",
    "    else:\n",
    "        padded_data = final_event\n",
    "    \n",
    "    lead_time = np.arange(0,max_len*6 ,6)\n",
    "    padded_data['lead'] = ('time', lead_time)\n",
    "    padded_data = padded_data.assign_coords({'lead': padded_data['lead'].astype(int)})\n",
    "    \n",
    "    # swap time and lead dimensions\n",
    "    padded_data = padded_data.swap_dims({'time': 'lead'})\n",
    "    padded_data['id'] = id_number \n",
    "    \n",
    "    # convert the time dimension to a variable\n",
    "    final_data.append(padded_data)\n",
    "\n",
    "\n",
    "final_input_padded = xr.concat(final_data, dim='SID')\n",
    "final_input_padded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def process_row(row, input_dsets):\n",
    "    lat = int(row['LAT'])\n",
    "    lon = int(row['LON'])\n",
    "    time = row['datetime']\n",
    "    latmin = lat - 5\n",
    "    latmax = lat + 5\n",
    "    lonmin = lon - 5\n",
    "    lonmax = lon + 5\n",
    "\n",
    "    try:\n",
    "        # Select the data for the given lat/lon/time\n",
    "        sel_data = input_dsets.sel(latitude=slice(latmax, latmin), longitude=slice(lonmin, lonmax), time=time)\n",
    "    \n",
    "    except KeyError:\n",
    "        # If data is not found, return None (will be filtered out later)\n",
    "        print(f\"Data not found for SID: {row['SID']} at time {time} with lat {lat} and lon {lon}\")\n",
    "        return None\n",
    "\n",
    "    # Add SID and wind speed as new variables\n",
    "    sel_data['id'] = row['id']\n",
    "    wind_speed = row['USA_WIND']\n",
    "\n",
    "    # Rename dimensions and set coordinate ranges\n",
    "    final_xr = sel_data.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "    final_xr['x'] = np.arange(0, len(final_xr['x']), 1)\n",
    "    final_xr['y'] = np.arange(0, len(final_xr['y']), 1)\n",
    "    final_xr = final_xr.fillna(0)  # Fill NaN values with zeros\n",
    "    final_xr['target'] = wind_speed\n",
    "\n",
    "    return final_xr\n",
    "\n",
    "# Wrap your row processing in dask.delayed\n",
    "delayed_results = []\n",
    "for index, row in ib_data_processed.iterrows():\n",
    "    delayed_result = dask.delayed(process_row)(row, input_dsets)\n",
    "    delayed_results.append(delayed_result)\n",
    "\n",
    "# Compute in parallel and filter out None results\n",
    "final_data = dask.compute(*delayed_results)\n",
    "final_data = [ds for ds in final_data if ds is not None]\n",
    "\n",
    "# Concatenate along 'time' dimension\n",
    "final_data_xr = xr.concat(final_data, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_input_padded.to_netcdf('~/ml-hurricane-intensity/test_folder/input_predictands.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ab457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numpy(dset):\n",
    "    #nump_values = dset.to_array().values\n",
    "    print()\n",
    "    nump_values = np.array(dset)\n",
    "    df_data = None\n",
    "              \n",
    "    return df_data,nump_values\n",
    "\n",
    "\n",
    "df, nump_values = convert_to_numpy(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_final = xr.open_dataset('~/ml-hurricane-intensity/test_folder/input_predictands.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4041ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the data with NaN values\n",
    "nan_mask = np.isnan(input_final['r'])\n",
    "# nan coords \n",
    "\n",
    "nan_coords = np.where(nan_mask)\n",
    "nan_coords_df = pd.DataFrame({'time': nan_coords[0], 'y': nan_coords[1], 'x': nan_coords[2], 'SID': input_final['SID'].values[nan_coords[0]]})\n",
    "\n",
    "nan_coords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7e8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_padded_filled = final_input_padded.fillna(0)  # Fill NaN values with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_final = final_padded_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = input_final[['u', 'v', 'vo', 'speed_shear', 'sp', 'r', 'cor_params']].to_array().values\n",
    "\n",
    "sid_groups = input_final['SID'].to_dataframe()\n",
    "\n",
    "\n",
    "x_data_ordered = x_data.transpose(1,2,3,4,0)\n",
    "\n",
    "y_data = input_final['target'].values\n",
    "\n",
    "\n",
    "# expand dimensions to match the input shape\n",
    "y_data = np.expand_dims(y_data, axis=-1)\n",
    "\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a774a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data_ordered, y_data, test_size=0.2, random_state=1)\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
    "x_test_scaled = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "# import timedistributed as tfd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Reshape\n",
    "from tensorflow.keras.layers import Masking\n",
    "# import adam\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# how to mask the data using tf cast not loss \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c000a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0.0), tf.float32)\n",
    "    squared_error = tf.square(y_true - y_pred)\n",
    "    masked_loss = tf.reduce_sum(squared_error * mask) / (tf.reduce_sum(mask) + 1e-6)\n",
    "    return masked_loss\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "    input_shape=(140, 5, 5, 7)\n",
    "))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(64, return_sequences=True))  # <--- important!\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', metrics=['mae'] , loss=masked_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ced5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d22541",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45f558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
